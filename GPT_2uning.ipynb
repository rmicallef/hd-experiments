{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GPT-2uning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmicallef/hd-experiments/blob/main/GPT_2uning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7iGg-j8Pg3J"
      },
      "source": [
        "Goals:\n",
        "\n",
        "Finetune GPT-2 on per-environment (i.e., genre) fan fiction corpus, and use hand-written metascenes to generate candidates\n",
        "\n",
        "Questions: How well does this work? What's the quality of the generated output?\n",
        "\n",
        "How specifically do we need to fine-tune the corpus? Can a single generator work for multiple genres?\n",
        "\n",
        "How do we use transformers here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cds_JHhgat0M"
      },
      "source": [
        "Let's see what DialoGPT looks like. I'm not sure it's going to be usefulfor anything other than NPC dialog, but it's worth seeing how well it holds up to button mashing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZzMP-G2dgfm"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe4p9ruCKwC2",
        "outputId": "10b17a83-e4e8-48d1-e699-ecef457dd500"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'transformers' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewCuHEY6L7GX",
        "outputId": "760acc39-04b3-4c3c-f1a6-93855172f9ac"
      },
      "source": [
        "cd transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjUd3O8ni6wy",
        "outputId": "e4853a35-df1a-4e72-ad23-630fc64239cf"
      },
      "source": [
        "# this will install packages that require restarting the kernel, so it will end up needing to be run again after kernel restart\n",
        "\n",
        "!pip install .\n",
        "#!pip install -r ./examples/requirements.txt\n",
        "#!pip install -I pyarrow==0.17.1 numpy==1.18.0 future==0.18.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (20.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (0.9.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.1.0.dev0) (0.17.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.1.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.1.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.1.0.dev0) (2.4.7)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.1.0.dev0-cp36-none-any.whl size=1448430 sha256=e73bccf8520a59df86648ddc323e5a821025704f942279dfa3efde9bffc35c72\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h2t9__mq/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Found existing installation: transformers 4.1.0.dev0\n",
            "    Uninstalling transformers-4.1.0.dev0:\n",
            "      Successfully uninstalled transformers-4.1.0.dev0\n",
            "Successfully installed transformers-4.1.0.dev0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "zK-u1BY0MVPr",
        "outputId": "a3ff5d40-9976-49c1-af19-fbcdd2fdd161"
      },
      "source": [
        "# DialoGPT Tests\n",
        "\n",
        "'''\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "\n",
        "\n",
        "# Let's chat for a few lines\n",
        "for step in range(5):\n",
        "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
        "    new_user_input_ids = tokenizer.encode(input(\">> \") + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "    # append the new user input tokens to the chat history\n",
        "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
        "\n",
        "    # generated a response while limiting the total chat history to 1000 tokens, \n",
        "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    # pretty print last ouput tokens from bot\n",
        "    print(\"God: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\nimport torch\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\\n\\n\\n# Let\\'s chat for a few lines\\nfor step in range(5):\\n    # encode the new user input, add the eos_token and return a tensor in Pytorch\\n    new_user_input_ids = tokenizer.encode(input(\">> \") + tokenizer.eos_token, return_tensors=\\'pt\\')\\n\\n    # append the new user input tokens to the chat history\\n    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\\n\\n    # generated a response while limiting the total chat history to 1000 tokens, \\n    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\\n\\n    # pretty print last ouput tokens from bot\\n    print(\"God: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFl8iLzTdELS",
        "outputId": "f8658419-86bc-496f-b53b-031fcefc6285"
      },
      "source": [
        "# Hardware lottery!\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Dec 11 15:12:58 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P8    33W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcKL8CNiPxgB",
        "outputId": "18e56d93-9a5d-4711-b4af-f0a6b738d260"
      },
      "source": [
        "# No dialog for now, just generation. Less talk, more rokk \n",
        "# Generate text with untuned model\n",
        "# Here, we'll use the command line version\n",
        "\n",
        "prompt = '\"The quick brown fox\"'\n",
        "temp = '0.8'\n",
        "num_return_sequences = '3'\n",
        "\n",
        "!python ./examples/text-generation/run_generation.py --model_type=gpt2 --model_name_or_path=gpt2 --prompt={prompt} --length=50 --temperature={temp} --num_return_sequences={num_return_sequences}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-11 15:13:04.304569: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "12/11/2020 15:13:09 - WARNING - __main__ -   device: cuda, n_gpu: 1, 16-bits training: False\n",
            "12/11/2020 15:13:44 - INFO - __main__ -   Namespace(device=device(type='cuda'), fp16=False, k=0, length=50, model_name_or_path='gpt2', model_type='gpt2', n_gpu=1, no_cuda=False, num_return_sequences=3, p=0.9, padding_text='', prefix='', prompt='The quick brown fox', repetition_penalty=1.0, seed=42, stop_token=None, temperature=0.8, xlm_language='')\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "=== GENERATED SEQUENCE 1 ===\n",
            "The quick brown foxes take advantage of this and lead the prey away with a clever move.\n",
            "\n",
            "This is an excellent time to take a close look at the prey, as they can make a good bait. The foxes are very protective of their prey, especially\n",
            "=== GENERATED SEQUENCE 2 ===\n",
            "The quick brown foxes follow the raised paws of the foxes and form a silhouette in front of the foxes. They begin to walk slowly towards the foxes.\n",
            "\n",
            "The foxes are not afraid of the foxes. They can walk faster than the fox\n",
            "=== GENERATED SEQUENCE 3 ===\n",
            "The quick brown fox was far better, but his roar was out of place. The cold sweat made it almost impossible to smile.\n",
            "\n",
            "\"Fuck, I'll kill him.\"\n",
            "\n",
            "The sound of the arm being removed from his throat was somewhat more subdued than before\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6aBpX7rXMSV"
      },
      "source": [
        "# Generate text with the code from inside the huggingface example\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from transformers import (\n",
        "    CTRLLMHeadModel,\n",
        "    CTRLTokenizer,\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer,\n",
        "    OpenAIGPTLMHeadModel,\n",
        "    OpenAIGPTTokenizer,\n",
        "    TransfoXLLMHeadModel,\n",
        "    TransfoXLTokenizer,\n",
        "    XLMTokenizer,\n",
        "    XLMWithLMHeadModel,\n",
        "    XLNetLMHeadModel,\n",
        "    XLNetTokenizer,\n",
        ")\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    \"gpt2\": (GPT2LMHeadModel, GPT2Tokenizer),\n",
        "    \"ctrl\": (CTRLLMHeadModel, CTRLTokenizer),\n",
        "    \"openai-gpt\": (OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
        "    \"xlnet\": (XLNetLMHeadModel, XLNetTokenizer),\n",
        "    \"transfo-xl\": (TransfoXLLMHeadModel, TransfoXLTokenizer),\n",
        "    \"xlm\": (XLMWithLMHeadModel, XLMTokenizer),\n",
        "}\n",
        "\n",
        "# Padding text to help Transformer-XL and XLNet with short prompts as proposed by Aman Rusia\n",
        "# in https://github.com/rusiaaman/XLNet-gen#methodology\n",
        "# and https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e\n",
        "PREFIX = \"\"\"In 1991, the remains of Russian Tsar Nicholas II and his family\n",
        "(except for Alexei and Maria) are discovered.\n",
        "The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\n",
        "remainder of the story. 1883 Western Siberia,\n",
        "a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
        "Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
        "father initially slaps him for making such an accusation, Rasputin watches as the\n",
        "man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
        "the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
        "with people, even a bishop, begging for his blessing. <eod> </s> <eos>\"\"\"\n",
        "\n",
        "\n",
        "def set_seed(args):\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "#\n",
        "# Functions to prepare models' input\n",
        "#\n",
        "\n",
        "\n",
        "def prepare_ctrl_input(args, _, tokenizer, prompt_text):\n",
        "    if args.temperature > 0.7:\n",
        "        logger.info(\"CTRL typically works better with lower temperatures (and lower top_k).\")\n",
        "\n",
        "    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False)\n",
        "    if not any(encoded_prompt[0] == x for x in tokenizer.control_codes.values()):\n",
        "        logger.info(\"WARNING! You are not starting your generation from a control code so you won't get good results\")\n",
        "    return prompt_text\n",
        "\n",
        "\n",
        "def prepare_xlm_input(args, model, tokenizer, prompt_text):\n",
        "    # kwargs = {\"language\": None, \"mask_token_id\": None}\n",
        "\n",
        "    # Set the language\n",
        "    use_lang_emb = hasattr(model.config, \"use_lang_emb\") and model.config.use_lang_emb\n",
        "    if hasattr(model.config, \"lang2id\") and use_lang_emb:\n",
        "        available_languages = model.config.lang2id.keys()\n",
        "        if args.xlm_language in available_languages:\n",
        "            language = args.xlm_language\n",
        "        else:\n",
        "            language = None\n",
        "            while language not in available_languages:\n",
        "                language = input(\"Using XLM. Select language in \" + str(list(available_languages)) + \" >>> \")\n",
        "\n",
        "        model.config.lang_id = model.config.lang2id[language]\n",
        "        # kwargs[\"language\"] = tokenizer.lang2id[language]\n",
        "\n",
        "    # TODO fix mask_token_id setup when configurations will be synchronized between models and tokenizers\n",
        "    # XLM masked-language modeling (MLM) models need masked token\n",
        "    # is_xlm_mlm = \"mlm\" in args.model_name_or_path\n",
        "    # if is_xlm_mlm:\n",
        "    #     kwargs[\"mask_token_id\"] = tokenizer.mask_token_id\n",
        "\n",
        "    return prompt_text\n",
        "\n",
        "\n",
        "def prepare_xlnet_input(args, _, tokenizer, prompt_text):\n",
        "    prefix = args.prefix if args.prefix else args.padding_text if args.padding_text else PREFIX\n",
        "    prompt_text = prefix + prompt_text\n",
        "    return prompt_text\n",
        "\n",
        "\n",
        "def prepare_transfoxl_input(args, _, tokenizer, prompt_text):\n",
        "    prefix = args.prefix if args.prefix else args.padding_text if args.padding_text else PREFIX\n",
        "    prompt_text = prefix + prompt_text\n",
        "    return prompt_text\n",
        "\n",
        "\n",
        "PREPROCESSING_FUNCTIONS = {\n",
        "    \"ctrl\": prepare_ctrl_input,\n",
        "    \"xlm\": prepare_xlm_input,\n",
        "    \"xlnet\": prepare_xlnet_input,\n",
        "    \"transfo-xl\": prepare_transfoxl_input,\n",
        "}\n",
        "\n",
        "\n",
        "def adjust_length_to_model(length, max_sequence_length):\n",
        "    if length < 0 and max_sequence_length > 0:\n",
        "        length = max_sequence_length\n",
        "    elif 0 < max_sequence_length < length:\n",
        "        length = max_sequence_length  # No generation bigger than model size\n",
        "    elif length < 0:\n",
        "        length = MAX_LENGTH  # avoid infinite loop\n",
        "    return length\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRGy4NHaap_M"
      },
      "source": [
        "### Main() from huggingface example code with a few modifications\n",
        "\n",
        "def remember_the_main(args_to_pass):\n",
        "    \n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        \"--model_type\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_name_or_path\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Path to pre-trained model or shortcut name selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\"--prompt\", type=str, default=\"\")\n",
        "    parser.add_argument(\"--length\", type=int, default=20)\n",
        "    parser.add_argument(\"--stop_token\", type=str, default=None, help=\"Token at which text generation is stopped\")\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--temperature\",\n",
        "        type=float,\n",
        "        default=1.0,\n",
        "        help=\"temperature of 1.0 has no effect, lower tend toward greedy sampling\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--repetition_penalty\", type=float, default=1.0, help=\"primarily useful for CTRL model; in that case, use 1.2\"\n",
        "    )\n",
        "    parser.add_argument(\"--k\", type=int, default=0)\n",
        "    parser.add_argument(\"--p\", type=float, default=0.9)\n",
        "\n",
        "    parser.add_argument(\"--prefix\", type=str, default=\"\", help=\"Text added prior to input.\")\n",
        "    parser.add_argument(\"--padding_text\", type=str, default=\"\", help=\"Deprecated, the use of `--prefix` is preferred.\")\n",
        "    parser.add_argument(\"--xlm_language\", type=str, default=\"\", help=\"Optional language when used with the XLM model.\")\n",
        "\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n",
        "    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
        "    parser.add_argument(\"--num_return_sequences\", type=int, default=1, help=\"The number of samples to generate.\")\n",
        "    parser.add_argument(\n",
        "        \"--fp16\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
        "    )\n",
        "    parser.add_argument('--verbosity', type=int, default=3, help='how chatty do you want your system output')\n",
        "\n",
        "    # modify the defaults with our own list\n",
        "    args = parser.parse_args(args_to_pass)\n",
        "\n",
        "    args.device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "    args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n",
        "\n",
        "    if args.verbosity >= 2: logger.info(\n",
        "        \"device: %s, n_gpu: %s, 16-bits training: %s\",\n",
        "        args.device,\n",
        "        args.n_gpu,\n",
        "        args.fp16,\n",
        "    )\n",
        "\n",
        "    set_seed(args)\n",
        "\n",
        "    # BUSINESS TIME\n",
        "    # Initialize the model and tokenizer\n",
        "    try:\n",
        "        args.model_type = args.model_type.lower()\n",
        "        model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
        "    except KeyError:\n",
        "        raise KeyError(\"the model {} you specified is not supported. You are welcome to add it and open a PR :)\")\n",
        "\n",
        "    tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path)\n",
        "    model = model_class.from_pretrained(args.model_name_or_path)\n",
        "    model.to(args.device)\n",
        "\n",
        "    if args.fp16:\n",
        "        model.half()\n",
        "\n",
        "    args.length = adjust_length_to_model(args.length, max_sequence_length=model.config.max_position_embeddings)\n",
        "    if args.verbosity >= 3: logger.info(args)\n",
        "\n",
        "    prompt_text = args.prompt if args.prompt else input(\"Model prompt >>> \")\n",
        "\n",
        "    # Different models need different input formatting and/or extra arguments\n",
        "    requires_preprocessing = args.model_type in PREPROCESSING_FUNCTIONS.keys()\n",
        "    if requires_preprocessing:\n",
        "        prepare_input = PREPROCESSING_FUNCTIONS.get(args.model_type)\n",
        "        preprocessed_prompt_text = prepare_input(args, model, tokenizer, prompt_text)\n",
        "\n",
        "        if model.__class__.__name__ in [\"TransfoXLLMHeadModel\"]:\n",
        "            tokenizer_kwargs = {\"add_space_before_punct_symbol\": True}\n",
        "        else:\n",
        "            tokenizer_kwargs = {}\n",
        "\n",
        "        encoded_prompt = tokenizer.encode(\n",
        "            preprocessed_prompt_text, add_special_tokens=False, return_tensors=\"pt\", **tokenizer_kwargs\n",
        "        )\n",
        "    else:\n",
        "        prefix = args.prefix if args.prefix else args.padding_text\n",
        "        encoded_prompt = tokenizer.encode(prefix + prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "    encoded_prompt = encoded_prompt.to(args.device)\n",
        "\n",
        "    if encoded_prompt.size()[-1] == 0:\n",
        "        input_ids = None\n",
        "    else:\n",
        "        input_ids = encoded_prompt\n",
        "\n",
        "    output_sequences = model.generate(input_ids=input_ids,    max_length=args.length + len(encoded_prompt[0]),    temperature=args.temperature,    top_k=args.k,    top_p=args.p,    repetition_penalty=args.repetition_penalty,    do_sample=True,    num_return_sequences=args.num_return_sequences,)\n",
        "\n",
        "    # Remove the batch dimension when returning multiple sequences\n",
        "    if len(output_sequences.shape) > 2:\n",
        "        output_sequences.squeeze_()\n",
        "\n",
        "    generated_sequences = []\n",
        "\n",
        "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "        if args.verbosity >= 3: print(\"=== GENERATED SEQUENCE {} ===\".format(generated_sequence_idx + 1)) \n",
        "        generated_sequence = generated_sequence.tolist()\n",
        "\n",
        "        # Decode text\n",
        "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "        # Remove all text after the stop token\n",
        "        text = text[: text.find(args.stop_token) if args.stop_token else None]\n",
        "\n",
        "        # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing\n",
        "        total_sequence = (\n",
        "            prompt_text + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
        "        )\n",
        "\n",
        "        generated_sequences.append(total_sequence)\n",
        "        if args.verbosity >= 1: print(total_sequence)\n",
        "\n",
        "    return generated_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-eAgL4JQIjY"
      },
      "source": [
        "# Text generation harness\n",
        "\n",
        "def format_args(args_dict):\n",
        "    args_list = []\n",
        "    for k, v in args_dict.items():\n",
        "        args_list.append('--{}'.format(k))\n",
        "        args_list.append(str(v))\n",
        "    return args_list\n",
        "\n",
        "def generate_text(prompts, models, args_dict):\n",
        "\n",
        "    for prompt in prompts:\n",
        "        args_dict['prompt'] = prompt\n",
        "        print('\\n\\nPROMPT: {}\\n'.format(prompt))\n",
        "\n",
        "        for model in models:\n",
        "            args_dict['model_name_or_path'] = model\n",
        "            \n",
        "            generated_sequences = remember_the_main(format_args(args_dict))\n",
        "\n",
        "            for i, sequence in enumerate(generated_sequences):\n",
        "                print('\\n** Model {}, seq {}:\\n{}\\n**'.format(model, i, sequence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AER1aQawqQhx",
        "outputId": "a61ea4c5-275b-4a7e-f6c2-6910d4f96ad9"
      },
      "source": [
        "# Now we use the inline function instead of the command line\n",
        "\n",
        "default_args_dict = {\n",
        "    'num_return_sequences': 1,\n",
        "    'length': 30,\n",
        "    'temp': 1.0,\n",
        "    'verbosity': 0,\n",
        "    'model_type': 'gpt2', #for now, we're just looking at GPT-2 based models\n",
        "}\n",
        "\n",
        "generate_text(['The quick brown fox'], ['gpt2'], default_args_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "PROMPT: The quick brown fox\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model gpt2, seq 0:\n",
            "The quick brown foxes you see aren't as big, but they have a hard time making it hard to get on with the grass up ahead. Once they get over\n",
            "**\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9pMxhSKQL8h",
        "outputId": "60392588-2752-472a-f8e5-908006e93d3f"
      },
      "source": [
        "# Let's vary the prompts a little\n",
        "\n",
        "test_prompts = [\n",
        "    'Harry opened the door and saw',\n",
        "    'The New York Times reported today that',\n",
        "    'The Daily Prophet reported today that',\n",
        "    \"Picard stepped off the shuttlecraft. He couldn't believe his eyes\",\n",
        "]\n",
        "\n",
        "generate_text(test_prompts, ['gpt2'], default_args_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "PROMPT: Harry opened the door and saw\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model gpt2, seq 0:\n",
            "Harry opened the door and saw none of the bandits who had waited in the street. They had been making their way back home on foot to help up the Auctie.\n",
            "\n",
            "**\n",
            "\n",
            "\n",
            "PROMPT: The New York Times reported today that\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model gpt2, seq 0:\n",
            "The New York Times reported today that for the past four years, the Chief Justice had suggested that some individuals making outrageous and unconstitutional anti-Semitic statements should be expelled from the Supreme Court.\n",
            "**\n",
            "\n",
            "\n",
            "PROMPT: The Daily Prophet reported today that\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model gpt2, seq 0:\n",
            "The Daily Prophet reported today that for the past four years, the Chief of Staff of the Supreme Government has been portrayed as a \"courier for all her colleagues, the most\n",
            "**\n",
            "\n",
            "\n",
            "PROMPT: Picard stepped off the shuttlecraft. He couldn't believe his eyes\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model gpt2, seq 0:\n",
            "Picard stepped off the shuttlecraft. He couldn't believe his eyes, the station was in such awful condition. \"It would have been wonderful to have seen you on time.\"\n",
            "\n",
            "The AOC looked at him\n",
            "**\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3jmRRZ3P_GR",
        "outputId": "2f80a5e2-414e-46de-fc57-94cda32e063f"
      },
      "source": [
        "# Now we tune model with a training corpus\n",
        "\n",
        "!python /content/transformers/examples/language-modeling/run_clm.py \\\n",
        "--output_dir=/content/models/gpt2-potter-micro-8epochs \\\n",
        "--model_type=gpt2 \\\n",
        "--model_name_or_path=/content/models/gpt2-potter-micro-3epochs \\\n",
        "--do_train \\\n",
        "--train_file=/content/data/potter-train-micro.txt \\\n",
        "--do_eval \\\n",
        "--validation_file=/content/data/potter-test-micro.txt \\\n",
        "--per_device_train_batch_size=1 \\\n",
        "--per_device_eval_batch_size=1 \\\n",
        "--learning_rate 5e-5 \\\n",
        "--num_train_epochs=5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-11 15:14:35.596535: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "12/11/2020 15:14:37 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "12/11/2020 15:14:37 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/content/models/gpt2-potter-micro-8epochs', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, model_parallel=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec11_15-14-37_82d91dacce52', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/content/models/gpt2-potter-micro-8epochs', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False)\n",
            "Using custom data configuration default\n",
            "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-55ecc883b38662ad/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
            "[INFO|configuration_utils.py:420] 2020-12-11 15:14:38,503 >> loading configuration file /content/models/gpt2-potter-micro-3epochs/config.json\n",
            "[INFO|configuration_utils.py:458] 2020-12-11 15:14:38,503 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"/content/models/gpt2-potter-micro-2epochs\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:420] 2020-12-11 15:14:38,504 >> loading configuration file /content/models/gpt2-potter-micro-3epochs/config.json\n",
            "[INFO|configuration_utils.py:458] 2020-12-11 15:14:38,504 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"/content/models/gpt2-potter-micro-2epochs\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1713] 2020-12-11 15:14:38,504 >> Model name '/content/models/gpt2-potter-micro-3epochs' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/models/gpt2-potter-micro-3epochs' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "[INFO|tokenization_utils_base.py:1746] 2020-12-11 15:14:38,505 >> Didn't find file /content/models/gpt2-potter-micro-3epochs/tokenizer.json. We won't load it.\n",
            "[INFO|tokenization_utils_base.py:1746] 2020-12-11 15:14:38,505 >> Didn't find file /content/models/gpt2-potter-micro-3epochs/added_tokens.json. We won't load it.\n",
            "[INFO|tokenization_utils_base.py:1791] 2020-12-11 15:14:38,505 >> loading file /content/models/gpt2-potter-micro-3epochs/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1791] 2020-12-11 15:14:38,505 >> loading file /content/models/gpt2-potter-micro-3epochs/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1791] 2020-12-11 15:14:38,505 >> loading file None\n",
            "[INFO|tokenization_utils_base.py:1791] 2020-12-11 15:14:38,505 >> loading file None\n",
            "[INFO|tokenization_utils_base.py:1791] 2020-12-11 15:14:38,505 >> loading file /content/models/gpt2-potter-micro-3epochs/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1791] 2020-12-11 15:14:38,505 >> loading file /content/models/gpt2-potter-micro-3epochs/tokenizer_config.json\n",
            "[INFO|modeling_utils.py:1012] 2020-12-11 15:14:38,685 >> loading weights file /content/models/gpt2-potter-micro-3epochs/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:1130] 2020-12-11 15:14:58,654 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1139] 2020-12-11 15:14:58,654 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /content/models/gpt2-potter-micro-3epochs.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            " 11% 2/18 [00:00<00:01, 15.16ba/s][WARNING|tokenization_utils_base.py:3217] 2020-12-11 15:14:58,965 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1549 > 1024). Running this sequence through the model will result in indexing errors\n",
            "100% 18/18 [00:01<00:00, 14.55ba/s]\n",
            "100% 12/12 [00:00<00:00, 15.39ba/s]\n",
            "100% 18/18 [00:02<00:00,  7.09ba/s]\n",
            "100% 12/12 [00:02<00:00,  5.74ba/s]\n",
            "[INFO|trainer.py:362] 2020-12-11 15:15:08,285 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:362] 2020-12-11 15:15:08,286 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:668] 2020-12-11 15:15:08,290 >> ***** Running training *****\n",
            "[INFO|trainer.py:669] 2020-12-11 15:15:08,290 >>   Num examples = 431\n",
            "[INFO|trainer.py:670] 2020-12-11 15:15:08,290 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:671] 2020-12-11 15:15:08,290 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:672] 2020-12-11 15:15:08,290 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:673] 2020-12-11 15:15:08,290 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:674] 2020-12-11 15:15:08,290 >>   Total optimization steps = 2155\n",
            "{'loss': 2.8796875, 'learning_rate': 3.839907192575406e-05, 'epoch': 1.160092807424594}\n",
            " 23% 500/2155 [06:17<20:47,  1.33it/s][INFO|trainer.py:1183] 2020-12-11 15:21:26,069 >> Saving model checkpoint to /content/models/gpt2-potter-micro-8epochs/checkpoint-500\n",
            "[INFO|configuration_utils.py:289] 2020-12-11 15:21:26,072 >> Configuration saved in /content/models/gpt2-potter-micro-8epochs/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-11 15:21:27,815 >> Model weights saved in /content/models/gpt2-potter-micro-8epochs/checkpoint-500/pytorch_model.bin\n",
            "{'loss': 2.864886474609375, 'learning_rate': 2.679814385150812e-05, 'epoch': 2.320185614849188}\n",
            " 46% 1000/2155 [12:40<14:33,  1.32it/s][INFO|trainer.py:1183] 2020-12-11 15:27:49,023 >> Saving model checkpoint to /content/models/gpt2-potter-micro-8epochs/checkpoint-1000\n",
            "[INFO|configuration_utils.py:289] 2020-12-11 15:27:49,024 >> Configuration saved in /content/models/gpt2-potter-micro-8epochs/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-11 15:27:50,876 >> Model weights saved in /content/models/gpt2-potter-micro-8epochs/checkpoint-1000/pytorch_model.bin\n",
            "{'loss': 2.765029541015625, 'learning_rate': 1.5197215777262181e-05, 'epoch': 3.480278422273782}\n",
            " 70% 1500/2155 [19:03<08:13,  1.33it/s][INFO|trainer.py:1183] 2020-12-11 15:34:11,735 >> Saving model checkpoint to /content/models/gpt2-potter-micro-8epochs/checkpoint-1500\n",
            "[INFO|configuration_utils.py:289] 2020-12-11 15:34:11,736 >> Configuration saved in /content/models/gpt2-potter-micro-8epochs/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-11 15:34:13,602 >> Model weights saved in /content/models/gpt2-potter-micro-8epochs/checkpoint-1500/pytorch_model.bin\n",
            "{'loss': 2.663052978515625, 'learning_rate': 3.596287703016241e-06, 'epoch': 4.640371229698376}\n",
            " 93% 2000/2155 [25:27<01:56,  1.33it/s][INFO|trainer.py:1183] 2020-12-11 15:40:35,593 >> Saving model checkpoint to /content/models/gpt2-potter-micro-8epochs/checkpoint-2000\n",
            "[INFO|configuration_utils.py:289] 2020-12-11 15:40:35,595 >> Configuration saved in /content/models/gpt2-potter-micro-8epochs/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-11 15:40:37,538 >> Model weights saved in /content/models/gpt2-potter-micro-8epochs/checkpoint-2000/pytorch_model.bin\n",
            "100% 2155/2155 [27:31<00:00,  1.32it/s][INFO|trainer.py:821] 2020-12-11 15:42:39,484 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'epoch': 5.0}\n",
            "100% 2155/2155 [27:31<00:00,  1.31it/s]\n",
            "[INFO|trainer.py:1183] 2020-12-11 15:42:39,518 >> Saving model checkpoint to /content/models/gpt2-potter-micro-8epochs\n",
            "[INFO|configuration_utils.py:289] 2020-12-11 15:42:39,520 >> Configuration saved in /content/models/gpt2-potter-micro-8epochs/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-11 15:42:41,431 >> Model weights saved in /content/models/gpt2-potter-micro-8epochs/pytorch_model.bin\n",
            "12/11/2020 15:42:41 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:1354] 2020-12-11 15:42:41,522 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1355] 2020-12-11 15:42:41,522 >>   Num examples = 315\n",
            "[INFO|trainer.py:1356] 2020-12-11 15:42:41,522 >>   Batch size = 1\n",
            "100% 315/315 [01:14<00:00,  4.23it/s]\n",
            "12/11/2020 15:43:56 - INFO - __main__ -   ***** Eval results *****\n",
            "12/11/2020 15:43:56 - INFO - __main__ -     perplexity = 24.450018064193586\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZtCytiaP5rY",
        "outputId": "ece625e0-285c-4d60-de7d-39214b27589f"
      },
      "source": [
        "# Generate text with a tuned model\n",
        "\n",
        "generate_text(test_prompts, ['/content/models/gpt2-potter-micro-1epoch'], default_args_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "PROMPT: Harry opened the door and saw\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model /content/models/gpt2-potter-micro-1epoch, seq 0:\n",
            "Harry opened the door and saw none of the Slytherins in the hall. He stopped in front of Harry and looked at Ginny with admiration. She was her usual face. \n",
            "**\n",
            "\n",
            "\n",
            "PROMPT: The New York Times reported today that\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model /content/models/gpt2-potter-micro-1epoch, seq 0:\n",
            "The New York Times reported today that for the past four years, the Chief Justice had suggested that some individuals making it hard for human beings to live would be forced to live outside the homes\n",
            "**\n",
            "\n",
            "\n",
            "PROMPT: The Daily Prophet reported today that\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model /content/models/gpt2-potter-micro-1epoch, seq 0:\n",
            "The Daily Prophet reported today that none of the 21 grandchildren who survived the fire had any memory of their grandfathers fall. The burning remains of the home were found by security at the\n",
            "**\n",
            "\n",
            "\n",
            "PROMPT: Picard stepped off the shuttlecraft. He couldn't believe his eyes\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model /content/models/gpt2-potter-micro-1epoch, seq 0:\n",
            "Picard stepped off the shuttlecraft. He couldn't believe his eyes, the station was in such awful condition. Sirius and the others were making it hard not to let things like that slip by. Once, the crew\n",
            "**\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dp6xPQq7qoT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a29874fa-a339-43a6-90c4-5e0d96a55585"
      },
      "source": [
        "# Now let's compare some models.\n",
        "\n",
        "test_prompts = [\n",
        "    'Harry opened the door and saw',\n",
        "    #'What have you done with my wand?',\n",
        "    'The New York Times reported today that',\n",
        "    'The Daily Prophet reported today that',\n",
        "    #'There was trouble afoot at Hogwarts yesterday.',\n",
        "    #'There was trouble afoot at Harvard yesterday.',\n",
        "    #'There was trouble afoot at Kennedy High School yesterday.'\n",
        "    #'Hermione was quite excited about tomorrow.',\n",
        "    #'Melissa was quite excited about tomorrow.',\n",
        "    \"Picard stepped off the shuttlecraft. He couldn't believe his eyes\",\n",
        "]\n",
        "\n",
        "models = [\n",
        "          'gpt2',\n",
        "          #'/content/models/gpt2-potter-micro-1epoch',\n",
        "          #'/content/models/gpt2-potter-micro-2epochs',\n",
        "          #'/content/models/gpt2-potter-micro-3epochs',\n",
        "          '/content/models/gpt2-potter-micro-8epochs',\n",
        "]\n",
        "\n",
        "generate_text(test_prompts, models, default_args_dict)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "PROMPT: Harry opened the door and saw\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model gpt2, seq 0:\n",
            "Harry opened the door and saw none of the bandits who had waited in the street. They had been making their way back home on foot to help up the Auctie.\n",
            "\n",
            "**\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model /content/models/gpt2-potter-micro-8epochs, seq 0:\n",
            "Harry opened the door and saw Mr. Weasley walking in. Harry turned and saw the cloaked figure walking towards him.Whats the matter?\n",
            "**\n",
            "\n",
            "\n",
            "PROMPT: The New York Times reported today that\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model gpt2, seq 0:\n",
            "The New York Times reported today that for the past four years, the Chief Justice had suggested that some individuals making outrageous and unconstitutional anti-Semitic statements should be expelled from the Supreme Court.\n",
            "**\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model /content/models/gpt2-potter-micro-8epochs, seq 0:\n",
            "The New York Times reported today that for the second straight year, the Chief Justice had suggested that some members of his court could be forced to live with a mentally ill spouse, even as\n",
            "**\n",
            "\n",
            "\n",
            "PROMPT: The Daily Prophet reported today that\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model gpt2, seq 0:\n",
            "The Daily Prophet reported today that for the past four years, the Chief of Staff of the Supreme Government has been portrayed as a \"courier for all her colleagues, the most\n",
            "**\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model /content/models/gpt2-potter-micro-8epochs, seq 0:\n",
            "The Daily Prophet reported today that none of the four members of the caliphate are being held in secret, except for Mokhtar al-Shehri, who will face trial this\n",
            "**\n",
            "\n",
            "\n",
            "PROMPT: Picard stepped off the shuttlecraft. He couldn't believe his eyes\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model gpt2, seq 0:\n",
            "Picard stepped off the shuttlecraft. He couldn't believe his eyes, the station was in such awful condition. \"It would have been wonderful to have seen you on time.\"\n",
            "\n",
            "The AOC looked at him\n",
            "**\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model /content/models/gpt2-potter-micro-8epochs, seq 0:\n",
            "Picard stepped off the shuttlecraft. He couldn't believe his eyes, the station was utterly alien. He'd had enough. When he was announced to be at sea, the sounds of all her passengers squawking\n",
            "**\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMfbIqw-R_u_"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyDXS0FnZ_4Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74e78678-a075-4927-b073-f2fd98d187d8"
      },
      "source": [
        "# Note that we're training on full and evaluating on micro - this won't hurt the output, just speed up the perplexity measurement at the cost of a little perplecity accuracy\n",
        "# JK, training on full and testing on micro risks overlap between the training set and the test set, which taints the perplexity score\n",
        "!python /content/transformers/examples/language-modeling/run_clm.py \\\n",
        "--output_dir=/content/models/gpt2-potter-full-3epochs \\\n",
        "--model_type=gpt2 \\\n",
        "--model_name_or_path=/content/models/gpt2-potter-full-1epoch \\\n",
        "--do_train \\\n",
        "--train_file=/content/data/potter-train-full.txt \\\n",
        "--do_eval \\\n",
        "--validation_file=/content/data/potter-test-full.txt \\\n",
        "--per_device_train_batch_size=1 \\\n",
        "--per_device_eval_batch_size=1 \\\n",
        "--learning_rate 5e-5 \\\n",
        "--num_train_epochs=2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-11 17:40:51.669781: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "12/11/2020 17:40:57 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "12/11/2020 17:40:57 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/content/models/gpt2-potter-full-3epochs', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, model_parallel=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=2, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec11_17-40-57_82d91dacce52', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/content/models/gpt2-potter-full-3epochs', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False)\n",
            "Using custom data configuration default\n",
            "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-7397e8ca4442e2e9/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
            "[INFO|configuration_utils.py:420] 2020-12-11 17:40:58,754 >> loading configuration file /content/models/gpt2-potter-full-1epoch/config.json\n",
            "[INFO|configuration_utils.py:458] 2020-12-11 17:40:58,755 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:420] 2020-12-11 17:40:58,755 >> loading configuration file /content/models/gpt2-potter-full-1epoch/config.json\n",
            "[INFO|configuration_utils.py:458] 2020-12-11 17:40:58,756 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1713] 2020-12-11 17:40:58,756 >> Model name '/content/models/gpt2-potter-full-1epoch' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/models/gpt2-potter-full-1epoch' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "[INFO|tokenization_utils_base.py:1746] 2020-12-11 17:40:58,756 >> Didn't find file /content/models/gpt2-potter-full-1epoch/tokenizer.json. We won't load it.\n",
            "[INFO|tokenization_utils_base.py:1746] 2020-12-11 17:40:58,756 >> Didn't find file /content/models/gpt2-potter-full-1epoch/added_tokens.json. We won't load it.\n",
            "[INFO|tokenization_utils_base.py:1791] 2020-12-11 17:40:58,757 >> loading file /content/models/gpt2-potter-full-1epoch/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1791] 2020-12-11 17:40:58,757 >> loading file /content/models/gpt2-potter-full-1epoch/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1791] 2020-12-11 17:40:58,757 >> loading file None\n",
            "[INFO|tokenization_utils_base.py:1791] 2020-12-11 17:40:58,757 >> loading file None\n",
            "[INFO|tokenization_utils_base.py:1791] 2020-12-11 17:40:58,757 >> loading file /content/models/gpt2-potter-full-1epoch/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1791] 2020-12-11 17:40:58,757 >> loading file /content/models/gpt2-potter-full-1epoch/tokenizer_config.json\n",
            "[INFO|modeling_utils.py:1012] 2020-12-11 17:40:58,978 >> loading weights file /content/models/gpt2-potter-full-1epoch/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:1130] 2020-12-11 17:41:19,191 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1139] 2020-12-11 17:41:19,191 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /content/models/gpt2-potter-full-1epoch.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "  0% 0/135 [00:00<?, ?ba/s][WARNING|tokenization_utils_base.py:3217] 2020-12-11 17:41:19,401 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1693 > 1024). Running this sequence through the model will result in indexing errors\n",
            "100% 135/135 [00:10<00:00, 12.96ba/s]\n",
            "100% 55/55 [00:04<00:00, 13.16ba/s]\n",
            "100% 135/135 [00:31<00:00,  4.29ba/s]\n",
            "100% 55/55 [00:12<00:00,  4.40ba/s]\n",
            "[INFO|trainer.py:362] 2020-12-11 17:42:30,274 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:362] 2020-12-11 17:42:30,275 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:668] 2020-12-11 17:42:30,279 >> ***** Running training *****\n",
            "[INFO|trainer.py:669] 2020-12-11 17:42:30,279 >>   Num examples = 3762\n",
            "[INFO|trainer.py:670] 2020-12-11 17:42:30,279 >>   Num Epochs = 2\n",
            "[INFO|trainer.py:671] 2020-12-11 17:42:30,279 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:672] 2020-12-11 17:42:30,280 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "[INFO|trainer.py:673] 2020-12-11 17:42:30,280 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:674] 2020-12-11 17:42:30,280 >>   Total optimization steps = 3762\n",
            "{'loss': 3.03282666015625, 'learning_rate': 4.335459861775651e-05, 'epoch': 0.2658160552897395}\n",
            " 13% 500/3762 [11:45<1:16:50,  1.41s/it][INFO|trainer.py:1183] 2020-12-11 17:54:16,104 >> Saving model checkpoint to /content/models/gpt2-potter-full-3epochs/checkpoint-500\n",
            "[INFO|configuration_utils.py:289] 2020-12-11 17:54:16,107 >> Configuration saved in /content/models/gpt2-potter-full-3epochs/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-11 17:54:17,896 >> Model weights saved in /content/models/gpt2-potter-full-3epochs/checkpoint-500/pytorch_model.bin\n",
            "{'loss': 3.05513037109375, 'learning_rate': 3.670919723551303e-05, 'epoch': 0.531632110579479}\n",
            " 27% 1000/3762 [23:34<1:04:42,  1.41s/it][INFO|trainer.py:1183] 2020-12-11 18:06:04,969 >> Saving model checkpoint to /content/models/gpt2-potter-full-3epochs/checkpoint-1000\n",
            "[INFO|configuration_utils.py:289] 2020-12-11 18:06:04,971 >> Configuration saved in /content/models/gpt2-potter-full-3epochs/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-11 18:06:06,743 >> Model weights saved in /content/models/gpt2-potter-full-3epochs/checkpoint-1000/pytorch_model.bin\n",
            "{'loss': 3.05702001953125, 'learning_rate': 3.0063795853269537e-05, 'epoch': 0.7974481658692185}\n",
            " 40% 1500/3762 [35:23<53:04,  1.41s/it][INFO|trainer.py:1183] 2020-12-11 18:17:53,718 >> Saving model checkpoint to /content/models/gpt2-potter-full-3epochs/checkpoint-1500\n",
            "[INFO|configuration_utils.py:289] 2020-12-11 18:17:53,720 >> Configuration saved in /content/models/gpt2-potter-full-3epochs/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-11 18:17:55,626 >> Model weights saved in /content/models/gpt2-potter-full-3epochs/checkpoint-1500/pytorch_model.bin\n",
            "{'loss': 3.055681884765625, 'learning_rate': 2.341839447102605e-05, 'epoch': 1.063264221158958}\n",
            " 53% 2000/3762 [47:12<41:22,  1.41s/it][INFO|trainer.py:1183] 2020-12-11 18:29:43,037 >> Saving model checkpoint to /content/models/gpt2-potter-full-3epochs/checkpoint-2000\n",
            "[INFO|configuration_utils.py:289] 2020-12-11 18:29:43,039 >> Configuration saved in /content/models/gpt2-potter-full-3epochs/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-11 18:29:44,907 >> Model weights saved in /content/models/gpt2-potter-full-3epochs/checkpoint-2000/pytorch_model.bin\n",
            "{'loss': 2.96857177734375, 'learning_rate': 1.6772993088782562e-05, 'epoch': 1.3290802764486975}\n",
            " 66% 2500/3762 [59:02<29:35,  1.41s/it][INFO|trainer.py:1183] 2020-12-11 18:41:32,543 >> Saving model checkpoint to /content/models/gpt2-potter-full-3epochs/checkpoint-2500\n",
            "[INFO|configuration_utils.py:289] 2020-12-11 18:41:32,545 >> Configuration saved in /content/models/gpt2-potter-full-3epochs/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-11 18:41:34,460 >> Model weights saved in /content/models/gpt2-potter-full-3epochs/checkpoint-2500/pytorch_model.bin\n",
            "{'loss': 2.963614013671875, 'learning_rate': 1.0127591706539077e-05, 'epoch': 1.594896331738437}\n",
            " 80% 3000/3762 [1:10:51<17:49,  1.40s/it][INFO|trainer.py:1183] 2020-12-11 18:53:21,643 >> Saving model checkpoint to /content/models/gpt2-potter-full-3epochs/checkpoint-3000\n",
            "[INFO|configuration_utils.py:289] 2020-12-11 18:53:21,645 >> Configuration saved in /content/models/gpt2-potter-full-3epochs/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-11 18:53:23,534 >> Model weights saved in /content/models/gpt2-potter-full-3epochs/checkpoint-3000/pytorch_model.bin\n",
            "{'loss': 2.956095703125, 'learning_rate': 3.4821903242955873e-06, 'epoch': 1.8607123870281765}\n",
            " 93% 3500/3762 [1:22:40<06:08,  1.41s/it][INFO|trainer.py:1183] 2020-12-11 19:05:10,763 >> Saving model checkpoint to /content/models/gpt2-potter-full-3epochs/checkpoint-3500\n",
            "[INFO|configuration_utils.py:289] 2020-12-11 19:05:10,765 >> Configuration saved in /content/models/gpt2-potter-full-3epochs/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-11 19:05:12,620 >> Model weights saved in /content/models/gpt2-potter-full-3epochs/checkpoint-3500/pytorch_model.bin\n",
            "100% 3762/3762 [1:28:55<00:00,  1.41s/it][INFO|trainer.py:821] 2020-12-11 19:11:25,547 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'epoch': 2.0}\n",
            "100% 3762/3762 [1:28:55<00:00,  1.42s/it]\n",
            "[INFO|trainer.py:1183] 2020-12-11 19:11:25,581 >> Saving model checkpoint to /content/models/gpt2-potter-full-3epochs\n",
            "[INFO|configuration_utils.py:289] 2020-12-11 19:11:25,583 >> Configuration saved in /content/models/gpt2-potter-full-3epochs/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-11 19:11:27,436 >> Model weights saved in /content/models/gpt2-potter-full-3epochs/pytorch_model.bin\n",
            "12/11/2020 19:11:27 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:1354] 2020-12-11 19:11:27,525 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1355] 2020-12-11 19:11:27,525 >>   Num examples = 1502\n",
            "[INFO|trainer.py:1356] 2020-12-11 19:11:27,525 >>   Batch size = 2\n",
            "100% 751/751 [05:54<00:00,  2.12it/s]\n",
            "12/11/2020 19:17:22 - INFO - __main__ -   ***** Eval results *****\n",
            "12/11/2020 19:17:22 - INFO - __main__ -     perplexity = 20.793423023586215\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYnRehVZuoGG",
        "outputId": "4720046d-95fc-4ba2-e48d-0449ea394036"
      },
      "source": [
        "#DistilGPT2\n",
        "\n",
        "!python /content/transformers/examples/language-modeling/run_clm.py \\\n",
        "--output_dir=/content/models/distilgpt2-potter-full-1epoch \\\n",
        "--model_type=gpt2 \\\n",
        "--model_name_or_path=distilgpt2 \\\n",
        "--do_train \\\n",
        "--train_file=/content/data/potter-train-full.txt \\\n",
        "--do_eval \\\n",
        "--validation_file=/content/data/potter-test-full.txt \\\n",
        "--per_device_train_batch_size=1 \\\n",
        "--per_device_eval_batch_size=1 \\\n",
        "--learning_rate 5e-5 \\\n",
        "--num_train_epochs=1"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-11 16:56:37.128284: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "12/11/2020 16:56:42 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "12/11/2020 16:56:42 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/content/models/distilgpt2-potter-full-1epoch', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, model_parallel=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec11_16-56-42_82d91dacce52', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/content/models/distilgpt2-potter-full-1epoch', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False)\n",
            "Using custom data configuration default\n",
            "Downloading and preparing dataset text/default-7397e8ca4442e2e9 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-7397e8ca4442e2e9/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-7397e8ca4442e2e9/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab. Subsequent calls will reuse this data.\n",
            "12/11/2020 16:56:44 - INFO - filelock -   Lock 139957189981184 acquired on /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631.lock\n",
            "[INFO|file_utils.py:1228] 2020-12-11 16:56:44,498 >> https://huggingface.co/distilgpt2/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpoi_v1rsx\n",
            "Downloading: 100% 762/762 [00:00<00:00, 664kB/s]\n",
            "[INFO|file_utils.py:1232] 2020-12-11 16:56:44,757 >> storing https://huggingface.co/distilgpt2/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "[INFO|file_utils.py:1235] 2020-12-11 16:56:44,758 >> creating metadata file for /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "12/11/2020 16:56:44 - INFO - filelock -   Lock 139957189981184 released on /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631.lock\n",
            "[INFO|configuration_utils.py:422] 2020-12-11 16:56:44,758 >> loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "[INFO|configuration_utils.py:458] 2020-12-11 16:56:44,759 >> Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:422] 2020-12-11 16:56:45,019 >> loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "[INFO|configuration_utils.py:458] 2020-12-11 16:56:45,020 >> Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "12/11/2020 16:56:45 - INFO - filelock -   Lock 139954426564168 acquired on /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f.lock\n",
            "[INFO|file_utils.py:1228] 2020-12-11 16:56:45,289 >> https://huggingface.co/distilgpt2/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpsbxfperi\n",
            "Downloading: 100% 1.04M/1.04M [00:00<00:00, 2.12MB/s]\n",
            "[INFO|file_utils.py:1232] 2020-12-11 16:56:46,050 >> storing https://huggingface.co/distilgpt2/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|file_utils.py:1235] 2020-12-11 16:56:46,050 >> creating metadata file for /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "12/11/2020 16:56:46 - INFO - filelock -   Lock 139954426564168 released on /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f.lock\n",
            "12/11/2020 16:56:46 - INFO - filelock -   Lock 139954426564280 acquired on /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n",
            "[INFO|file_utils.py:1228] 2020-12-11 16:56:46,316 >> https://huggingface.co/distilgpt2/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmph15bj13w\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 1.12MB/s]\n",
            "[INFO|file_utils.py:1232] 2020-12-11 16:56:46,987 >> storing https://huggingface.co/distilgpt2/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1235] 2020-12-11 16:56:46,987 >> creating metadata file for /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "12/11/2020 16:56:46 - INFO - filelock -   Lock 139954426564280 released on /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n",
            "12/11/2020 16:56:47 - INFO - filelock -   Lock 139954426562656 acquired on /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0.lock\n",
            "[INFO|file_utils.py:1228] 2020-12-11 16:56:47,260 >> https://huggingface.co/distilgpt2/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmppn31u0e5\n",
            "Downloading: 100% 1.36M/1.36M [00:00<00:00, 2.71MB/s]\n",
            "[INFO|file_utils.py:1232] 2020-12-11 16:56:48,031 >> storing https://huggingface.co/distilgpt2/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|file_utils.py:1235] 2020-12-11 16:56:48,031 >> creating metadata file for /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "12/11/2020 16:56:48 - INFO - filelock -   Lock 139954426562656 released on /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0.lock\n",
            "[INFO|tokenization_utils_base.py:1793] 2020-12-11 16:56:48,032 >> loading file https://huggingface.co/distilgpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1793] 2020-12-11 16:56:48,032 >> loading file https://huggingface.co/distilgpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1793] 2020-12-11 16:56:48,032 >> loading file https://huggingface.co/distilgpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "12/11/2020 16:56:48 - INFO - filelock -   Lock 139954426564280 acquired on /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba.lock\n",
            "[INFO|file_utils.py:1228] 2020-12-11 16:56:48,370 >> https://huggingface.co/distilgpt2/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpkwlzf_ed\n",
            "Downloading: 100% 353M/353M [00:06<00:00, 56.4MB/s]\n",
            "[INFO|file_utils.py:1232] 2020-12-11 16:56:54,734 >> storing https://huggingface.co/distilgpt2/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba\n",
            "[INFO|file_utils.py:1235] 2020-12-11 16:56:54,735 >> creating metadata file for /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba\n",
            "12/11/2020 16:56:54 - INFO - filelock -   Lock 139954426564280 released on /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba.lock\n",
            "[INFO|modeling_utils.py:1014] 2020-12-11 16:56:54,736 >> loading weights file https://huggingface.co/distilgpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba\n",
            "[INFO|modeling_utils.py:1130] 2020-12-11 16:56:59,119 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1139] 2020-12-11 16:56:59,119 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "  0% 0/135 [00:00<?, ?ba/s][WARNING|tokenization_utils_base.py:3217] 2020-12-11 16:56:59,308 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1693 > 1024). Running this sequence through the model will result in indexing errors\n",
            "100% 135/135 [00:10<00:00, 13.11ba/s]\n",
            "100% 55/55 [00:04<00:00, 13.31ba/s]\n",
            "100% 135/135 [00:31<00:00,  4.31ba/s]\n",
            "100% 55/55 [00:12<00:00,  4.49ba/s]\n",
            "[INFO|trainer.py:362] 2020-12-11 16:58:00,353 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:362] 2020-12-11 16:58:00,354 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:668] 2020-12-11 16:58:00,356 >> ***** Running training *****\n",
            "[INFO|trainer.py:669] 2020-12-11 16:58:00,357 >>   Num examples = 3762\n",
            "[INFO|trainer.py:670] 2020-12-11 16:58:00,357 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:671] 2020-12-11 16:58:00,357 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:672] 2020-12-11 16:58:00,357 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:673] 2020-12-11 16:58:00,357 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:674] 2020-12-11 16:58:00,357 >>   Total optimization steps = 3762\n",
            "{'loss': 3.526814453125, 'learning_rate': 4.335459861775651e-05, 'epoch': 0.13290802764486975}\n",
            " 13% 500/3762 [03:51<25:05,  2.17it/s][INFO|trainer.py:1183] 2020-12-11 17:01:52,323 >> Saving model checkpoint to /content/models/distilgpt2-potter-full-1epoch/checkpoint-500\n",
            "[INFO|configuration_utils.py:289] 2020-12-11 17:01:52,325 >> Configuration saved in /content/models/distilgpt2-potter-full-1epoch/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-11 17:01:53,850 >> Model weights saved in /content/models/distilgpt2-potter-full-1epoch/checkpoint-500/pytorch_model.bin\n",
            "{'loss': 3.478381591796875, 'learning_rate': 3.670919723551303e-05, 'epoch': 0.2658160552897395}\n",
            " 27% 1000/3762 [07:47<21:18,  2.16it/s][INFO|trainer.py:1183] 2020-12-11 17:05:47,490 >> Saving model checkpoint to /content/models/distilgpt2-potter-full-1epoch/checkpoint-1000\n",
            "[INFO|configuration_utils.py:289] 2020-12-11 17:05:47,492 >> Configuration saved in /content/models/distilgpt2-potter-full-1epoch/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-11 17:05:48,962 >> Model weights saved in /content/models/distilgpt2-potter-full-1epoch/checkpoint-1000/pytorch_model.bin\n",
            "{'loss': 3.44983447265625, 'learning_rate': 3.0063795853269537e-05, 'epoch': 0.39872408293460926}\n",
            " 40% 1500/3762 [11:42<17:22,  2.17it/s][INFO|trainer.py:1183] 2020-12-11 17:09:42,527 >> Saving model checkpoint to /content/models/distilgpt2-potter-full-1epoch/checkpoint-1500\n",
            "[INFO|configuration_utils.py:289] 2020-12-11 17:09:42,529 >> Configuration saved in /content/models/distilgpt2-potter-full-1epoch/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-11 17:09:43,981 >> Model weights saved in /content/models/distilgpt2-potter-full-1epoch/checkpoint-1500/pytorch_model.bin\n",
            "{'loss': 3.402380615234375, 'learning_rate': 2.341839447102605e-05, 'epoch': 0.531632110579479}\n",
            " 53% 2000/3762 [15:37<13:33,  2.17it/s][INFO|trainer.py:1183] 2020-12-11 17:13:37,566 >> Saving model checkpoint to /content/models/distilgpt2-potter-full-1epoch/checkpoint-2000\n",
            "[INFO|configuration_utils.py:289] 2020-12-11 17:13:37,568 >> Configuration saved in /content/models/distilgpt2-potter-full-1epoch/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-11 17:13:39,035 >> Model weights saved in /content/models/distilgpt2-potter-full-1epoch/checkpoint-2000/pytorch_model.bin\n",
            "{'loss': 3.382369140625, 'learning_rate': 1.6772993088782562e-05, 'epoch': 0.6645401382243488}\n",
            " 66% 2500/3762 [19:32<09:43,  2.16it/s][INFO|trainer.py:1183] 2020-12-11 17:17:32,585 >> Saving model checkpoint to /content/models/distilgpt2-potter-full-1epoch/checkpoint-2500\n",
            "[INFO|configuration_utils.py:289] 2020-12-11 17:17:32,586 >> Configuration saved in /content/models/distilgpt2-potter-full-1epoch/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-11 17:17:34,017 >> Model weights saved in /content/models/distilgpt2-potter-full-1epoch/checkpoint-2500/pytorch_model.bin\n",
            "{'loss': 3.353982177734375, 'learning_rate': 1.0127591706539077e-05, 'epoch': 0.7974481658692185}\n",
            " 80% 3000/3762 [23:27<05:51,  2.17it/s][INFO|trainer.py:1183] 2020-12-11 17:21:27,638 >> Saving model checkpoint to /content/models/distilgpt2-potter-full-1epoch/checkpoint-3000\n",
            "[INFO|configuration_utils.py:289] 2020-12-11 17:21:27,640 >> Configuration saved in /content/models/distilgpt2-potter-full-1epoch/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-11 17:21:29,088 >> Model weights saved in /content/models/distilgpt2-potter-full-1epoch/checkpoint-3000/pytorch_model.bin\n",
            "{'loss': 3.365218017578125, 'learning_rate': 3.4821903242955873e-06, 'epoch': 0.9303561935140883}\n",
            " 93% 3500/3762 [27:22<02:01,  2.15it/s][INFO|trainer.py:1183] 2020-12-11 17:25:22,630 >> Saving model checkpoint to /content/models/distilgpt2-potter-full-1epoch/checkpoint-3500\n",
            "[INFO|configuration_utils.py:289] 2020-12-11 17:25:22,632 >> Configuration saved in /content/models/distilgpt2-potter-full-1epoch/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-11 17:25:24,054 >> Model weights saved in /content/models/distilgpt2-potter-full-1epoch/checkpoint-3500/pytorch_model.bin\n",
            "100% 3762/3762 [29:27<00:00,  2.17it/s][INFO|trainer.py:821] 2020-12-11 17:27:27,796 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'epoch': 1.0}\n",
            "100% 3762/3762 [29:27<00:00,  2.13it/s]\n",
            "[INFO|trainer.py:1183] 2020-12-11 17:27:27,824 >> Saving model checkpoint to /content/models/distilgpt2-potter-full-1epoch\n",
            "[INFO|configuration_utils.py:289] 2020-12-11 17:27:27,825 >> Configuration saved in /content/models/distilgpt2-potter-full-1epoch/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-11 17:27:29,218 >> Model weights saved in /content/models/distilgpt2-potter-full-1epoch/pytorch_model.bin\n",
            "12/11/2020 17:27:29 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:1354] 2020-12-11 17:27:29,302 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1355] 2020-12-11 17:27:29,302 >>   Num examples = 1502\n",
            "[INFO|trainer.py:1356] 2020-12-11 17:27:29,302 >>   Batch size = 1\n",
            "100% 1502/1502 [03:33<00:00,  7.04it/s]\n",
            "12/11/2020 17:31:02 - INFO - __main__ -   ***** Eval results *****\n",
            "12/11/2020 17:31:02 - INFO - __main__ -     perplexity = 27.002042308416907\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8YJ8nDIsuT1",
        "outputId": "94d31c8b-8c73-4049-f376-2cb01c2374e0"
      },
      "source": [
        "models = [\n",
        "          'gpt2',\n",
        "          #'/content/models/gpt2-potter-micro-1epoch',\n",
        "          #'/content/models/gpt2-potter-micro-2epochs',\n",
        "          #'/content/models/gpt2-potter-micro-3epochs',\n",
        "          '/content/models/gpt2-potter-micro-8epochs',\n",
        "          '/content/models/gpt2-potter-full-1epoch',\n",
        "          '/content/models/distilgpt2-potter-full-1epoch'\n",
        "]\n",
        "\n",
        "generate_text(test_prompts, models, default_args_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "PROMPT: Harry opened the door and saw\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model gpt2, seq 0:\n",
            "Harry opened the door and saw none of the bandits who had waited in the street. They had been making their way back home on foot to help up the Auctie.\n",
            "\n",
            "**\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model /content/models/gpt2-potter-micro-8epochs, seq 0:\n",
            "Harry opened the door and saw Mr. Weasley walking in. Harry turned and saw the cloaked figure walking towards him.Whats the matter?\n",
            "**\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model /content/models/gpt2-potter-full-1epoch, seq 0:\n",
            "Harry opened the door and saw none of them.\"They?\" Hermione called. \"I don't think it's anyone.\" \"Well, it's all her fault, but this\n",
            "**\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model /content/models/distilgpt2-potter-full-1epoch, seq 0:\n",
            "Harry opened the door and saw James sit down and clasped Harrys arm. James, he said with a smirk. He\n",
            "**\n",
            "\n",
            "\n",
            "PROMPT: The New York Times reported today that\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model gpt2, seq 0:\n",
            "The New York Times reported today that for the past four years, the Chief Justice had suggested that some individuals making outrageous and unconstitutional anti-Semitic statements should be expelled from the Supreme Court.\n",
            "**\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model /content/models/gpt2-potter-micro-8epochs, seq 0:\n",
            "The New York Times reported today that for the second straight year, the Chief Justice had suggested that some members of his court could be forced to live with a mentally ill spouse, even as\n",
            "**\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH5lTPC-ZncR"
      },
      "source": [
        "'''\n",
        "Notes:\n",
        "- tuning gpt2 on potter-micro (107 texts) takes about 6 minutes per epoch on a Tesla K80\n",
        "- tuning gpt2 on potter-micro (107 texts) takes about 50 minutes per epoch without a GPU\n",
        "- tuning gpt2 on potter-full (807 texts) takes about 50 minutes per epoch on a Tesla K80 - 1-epoch perplexity 18.88\n",
        "- tuning distilgpt2 on potter-full (807 texts) takes about 39 minutes per epoch on a Tesla K80 - 1-epoch perplexity 27.00\n",
        "\n",
        "\n",
        "Next steps\n",
        "- address the padding warning\n",
        "- try another corpus\n",
        "- try another model family?\n",
        "- try a template?\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}