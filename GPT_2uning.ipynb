{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GPT-2uning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmicallef/hd-experiments/blob/main/GPT_2uning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7iGg-j8Pg3J"
      },
      "source": [
        "Goals:\n",
        "\n",
        "Finetune GPT-2 on per-environment (i.e., genre) fan fiction corpus, and use hand-written metascenes to generate candidates\n",
        "\n",
        "Questions: How well does this work? What's the quality of the generated output?\n",
        "\n",
        "How specifically do we need to fine-tune the corpus? Can a single generator work for multiple genres?\n",
        "\n",
        "How do we use transformers here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cds_JHhgat0M"
      },
      "source": [
        "Let's see what DialoGPT looks like. I'm not sure it's going to be usefulfor anything other than NPC dialog, but it's worth seeing how well it holds up to button mashing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZzMP-G2dgfm"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe4p9ruCKwC2",
        "outputId": "e72ded3e-155d-4015-80c4-61d1f6723eef"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'transformers' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewCuHEY6L7GX",
        "outputId": "73e946e8-e8b2-48e3-a217-5b0268330b16"
      },
      "source": [
        "cd transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjUd3O8ni6wy",
        "outputId": "984944b2-1c4c-4ef3-d347-a259c2902937"
      },
      "source": [
        "# this will install packages that require restarting the kernel, so it will end up needing to be run again after a kernel restart\n",
        "\n",
        "!pip install .\n",
        "#!pip install -r ./examples/requirements.txt\n",
        "#!pip install -I pyarrow==0.17.1 numpy==1.18.0 future==0.18.1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (0.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (20.7)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (0.9.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.1.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.1.0.dev0) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.1.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.1.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.0.dev0) (2.10)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.1.0.dev0-cp36-none-any.whl size=1448634 sha256=f6d058037512a382f989e05687d036ad580abc8d85897f2260d9c8bb12c6a901\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nabr786x/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Found existing installation: transformers 4.1.0.dev0\n",
            "    Uninstalling transformers-4.1.0.dev0:\n",
            "      Successfully uninstalled transformers-4.1.0.dev0\n",
            "Successfully installed transformers-4.1.0.dev0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFl8iLzTdELS",
        "outputId": "8006f178-92a3-49d3-849e-8ccf5e12ca29"
      },
      "source": [
        "# Hardware lottery!\n",
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Dec 12 01:13:02 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P8    11W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "zK-u1BY0MVPr",
        "outputId": "9fff146a-5f46-43a1-993c-c2e44903690c"
      },
      "source": [
        "# DialoGPT Tests\n",
        "\n",
        "'''\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "\n",
        "\n",
        "# Let's chat for a few lines\n",
        "for step in range(5):\n",
        "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
        "    new_user_input_ids = tokenizer.encode(input(\">> \") + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "    # append the new user input tokens to the chat history\n",
        "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
        "\n",
        "    # generated a response while limiting the total chat history to 1000 tokens, \n",
        "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    # pretty print last ouput tokens from bot\n",
        "    print(\"God: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
        "'''"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\nimport torch\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\\n\\n\\n# Let\\'s chat for a few lines\\nfor step in range(5):\\n    # encode the new user input, add the eos_token and return a tensor in Pytorch\\n    new_user_input_ids = tokenizer.encode(input(\">> \") + tokenizer.eos_token, return_tensors=\\'pt\\')\\n\\n    # append the new user input tokens to the chat history\\n    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\\n\\n    # generated a response while limiting the total chat history to 1000 tokens, \\n    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\\n\\n    # pretty print last ouput tokens from bot\\n    print(\"God: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcKL8CNiPxgB",
        "outputId": "d9abf5d3-2f06-48c8-b3ac-16c61bab6ae8"
      },
      "source": [
        "# No dialog for now, just generation. Less talk, more rokk \n",
        "# Generate text with untuned model\n",
        "# Here, we'll use the command line version\n",
        "\n",
        "prompt = '\"The quick brown fox\"'\n",
        "temp = '0.8'\n",
        "num_return_sequences = '3'\n",
        "\n",
        "!python ./examples/text-generation/run_generation.py --model_type=gpt2 --model_name_or_path=gpt2 --prompt={prompt} --length=50 --temperature={temp} --num_return_sequences={num_return_sequences}"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-12 00:47:11.476827: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "12/12/2020 00:47:16 - WARNING - __main__ -   device: cuda, n_gpu: 1, 16-bits training: False\n",
            "12/12/2020 00:47:50 - INFO - __main__ -   Namespace(device=device(type='cuda'), fp16=False, k=0, length=50, model_name_or_path='gpt2', model_type='gpt2', n_gpu=1, no_cuda=False, num_return_sequences=3, p=0.9, padding_text='', prefix='', prompt='The quick brown fox', repetition_penalty=1.0, seed=42, stop_token=None, temperature=0.8, xlm_language='')\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "=== GENERATED SEQUENCE 1 ===\n",
            "The quick brown foxes you see in the picture below, are actually the offspring of an eucalyptus. The eucalyptus is a fungus which grows in the plants around them and holds the bugs in place. The black and white eucaly\n",
            "=== GENERATED SEQUENCE 2 ===\n",
            "The quick brown fox is like this: she is clean and has a nice face. But, the fox is no longer clean. It's a normal fox. In fact, this fox is so clean that it was in fact taken for granted by the trainers.\n",
            "\n",
            "\n",
            "\n",
            "=== GENERATED SEQUENCE 3 ===\n",
            "The quick brown foxes were sent to their destination. It was a clear day and the sun was out.\n",
            "\n",
            "\"You guys are good to go. It's good to see your friends and family. We can't wait to see your movie. Can't wait\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6aBpX7rXMSV"
      },
      "source": [
        "# Generate text with the code with a streamlined version of the huggingface example\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from transformers import GPT2LMHeadModel,GPT2Tokenizer\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
        "\n",
        "def set_seed(args):\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "#\n",
        "# Functions to prepare models' input\n",
        "#\n",
        "\n",
        "def adjust_length_to_model(length, max_sequence_length):\n",
        "    if length < 0 and max_sequence_length > 0:\n",
        "        length = max_sequence_length\n",
        "    elif 0 < max_sequence_length < length:\n",
        "        length = max_sequence_length  # No generation bigger than model size\n",
        "    elif length < 0:\n",
        "        length = MAX_LENGTH  # avoid infinite loop\n",
        "    return length\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRGy4NHaap_M"
      },
      "source": [
        "### Main() from huggingface example code with a few modifications\n",
        "\n",
        "def remember_the_main(args_to_pass):\n",
        "    \n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model_type\", type=str, default='gpt2')\n",
        "    parser.add_argument(\"--model_name_or_path\", type=str, default=None, required=True)\n",
        "    parser.add_argument(\"--prompt\", type=str, default=\"\")\n",
        "    parser.add_argument(\"--length\", type=int, default=20)\n",
        "    parser.add_argument(\"--stop_token\", type=str, default=None, help=\"Token at which text generation is stopped\")\n",
        "    parser.add_argument(\"--temperature\", type=float, default=1.0, help=\"temperature of 1.0 has no effect, lower tend toward greedy sampling\")\n",
        "    parser.add_argument(\"--repetition_penalty\", type=float, default=1.0, help=\"primarily useful for CTRL model; in that case, use 1.2\")\n",
        "    parser.add_argument(\"--k\", type=int, default=0)\n",
        "    parser.add_argument(\"--p\", type=float, default=0.9)\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n",
        "    parser.add_argument(\"--num_return_sequences\", type=int, default=1, help=\"The number of samples to generate.\")\n",
        "    args = parser.parse_args(args_to_pass)\n",
        "\n",
        "    args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    args.n_gpu = torch.cuda.device_count()\n",
        "\n",
        "    #logger.warning(\"device: %s, n_gpu: %s\", args.device, args.n_gpu)\n",
        "\n",
        "    set_seed(args)\n",
        "\n",
        "    # Initialize the model and tokenizer\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(args.model_name_or_path)\n",
        "    model = GPT2LMHeadModel.from_pretrained(args.model_name_or_path)\n",
        "    model.to(args.device)\n",
        "\n",
        "    args.length = adjust_length_to_model(args.length, max_sequence_length=model.config.max_position_embeddings)\n",
        "    #logger.info(args)\n",
        "\n",
        "    prompt_text = args.prompt if args.prompt else input(\"Model prompt >>> \")\n",
        "\n",
        "    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "    encoded_prompt = encoded_prompt.to(args.device)\n",
        "\n",
        "    if encoded_prompt.size()[-1] == 0:\n",
        "        input_ids = None\n",
        "    else:\n",
        "        input_ids = encoded_prompt\n",
        "\n",
        "    output_sequences = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=args.length + len(encoded_prompt[0]),\n",
        "        temperature=args.temperature,\n",
        "        top_k=args.k,\n",
        "        top_p=args.p,\n",
        "        repetition_penalty=args.repetition_penalty,\n",
        "        do_sample=True,\n",
        "        num_return_sequences=args.num_return_sequences,\n",
        "    )\n",
        "\n",
        "    # Remove the batch dimension when returning multiple sequences\n",
        "    if len(output_sequences.shape) > 2:\n",
        "        output_sequences.squeeze_()\n",
        "\n",
        "    generated_sequences = []\n",
        "\n",
        "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "        #print(\"=== GENERATED SEQUENCE {} ===\".format(generated_sequence_idx + 1))\n",
        "        generated_sequence = generated_sequence.tolist()\n",
        "\n",
        "        # Decode text\n",
        "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "        # Remove all text after the stop token\n",
        "        text = text[: text.find(args.stop_token) if args.stop_token else None]\n",
        "\n",
        "        # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing\n",
        "        total_sequence = (\n",
        "            prompt_text + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
        "        )\n",
        "\n",
        "        generated_sequences.append(total_sequence)\n",
        "        #print(total_sequence)\n",
        "\n",
        "    return generated_sequences\n",
        "    \n",
        "    "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-eAgL4JQIjY"
      },
      "source": [
        "# Text generation helpers\n",
        "\n",
        "def format_args(args_dict):\n",
        "    args_list = []\n",
        "    for k, v in args_dict.items():\n",
        "        args_list.append('--{}'.format(k))\n",
        "        args_list.append(str(v))\n",
        "    return args_list\n",
        "\n",
        "def generate_text(prompts, models, args_dict):\n",
        "\n",
        "    for prompt in prompts:\n",
        "        args_dict['prompt'] = prompt\n",
        "        print('\\n\\nPROMPT: {}\\n'.format(prompt))\n",
        "\n",
        "        for model in models:\n",
        "            args_dict['model_name_or_path'] = model\n",
        "            \n",
        "            generated_sequences = remember_the_main(format_args(args_dict))\n",
        "\n",
        "            for i, sequence in enumerate(generated_sequences):\n",
        "                print('\\n** Model {}, seq {}:\\n{}\\n**'.format(model, i, sequence))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AER1aQawqQhx",
        "outputId": "a9cca4a6-14f0-40fc-8418-0efa3fc46163"
      },
      "source": [
        "# Now we use the inline function instead of the command line\n",
        "\n",
        "default_args_dict = {\n",
        "    'num_return_sequences': 1,\n",
        "    'length': 30,\n",
        "    'temp': 1.0,\n",
        "    'model_type': 'gpt2', #for now, we're just looking at GPT-2 based models\n",
        "}\n",
        "\n",
        "generate_text(['The quick brown fox'], ['gpt2'], default_args_dict)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "PROMPT: The quick brown fox\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model gpt2, seq 0:\n",
            "The quick brown foxes you see aren't as big, but they have a hard time making it hard to get on with the grass up ahead. Once they get over\n",
            "**\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9pMxhSKQL8h",
        "outputId": "75e2302c-186c-4819-e328-4689f6ca9e8b"
      },
      "source": [
        "# Let's vary the prompts a little\n",
        "\n",
        "test_prompts = [\n",
        "    'Harry opened the door and saw',\n",
        "    'The New York Times reported today that',\n",
        "    'The Daily Prophet reported today that',\n",
        "    \"Picard stepped off the shuttlecraft. He couldn't believe his eyes\",\n",
        "]\n",
        "\n",
        "generate_text(test_prompts, ['gpt2'], default_args_dict)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "PROMPT: Harry opened the door and saw\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model gpt2, seq 0:\n",
            "Harry opened the door and saw none of the girls in the house, but had heard the voice from the edge of the window.\n",
            "\n",
            "What was all this about, Dorner\n",
            "**\n",
            "\n",
            "\n",
            "PROMPT: The New York Times reported today that\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model gpt2, seq 0:\n",
            "The New York Times reported today that for the past four years, the Chief Justice had overseen the Supreme Court's quest to regulate human experimentation in the United States. A New York federal court\n",
            "**\n",
            "\n",
            "\n",
            "PROMPT: The Daily Prophet reported today that\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model gpt2, seq 0:\n",
            "The Daily Prophet reported today that for the past four years, the Chief Rabbi had suggested that some Jews who have not received sabbat could end up starting her wedding, but this\n",
            "**\n",
            "\n",
            "\n",
            "PROMPT: Picard stepped off the shuttlecraft. He couldn't believe his eyes\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model gpt2, seq 0:\n",
            "Picard stepped off the shuttlecraft. He couldn't believe his eyes, the station was in lockdown, and he had never seen anything like it before.\n",
            "\n",
            "\"Well, when you all go down, expect this\n",
            "**\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3jmRRZ3P_GR",
        "outputId": "865796ec-5bf7-4d1f-df5a-5730449f0be4"
      },
      "source": [
        "# Now we tune the model with a training corpus\n",
        "\n",
        "!python /content/transformers/examples/language-modeling/run_clm.py \\\n",
        "--model_type=gpt2 \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--per_device_train_batch_size=1 \\\n",
        "--per_device_eval_batch_size=1 \\\n",
        "--learning_rate 5e-5 \\\n",
        "--save_steps 5000 \\\n",
        "\\\n",
        "--model_name_or_path=gpt2 \\\n",
        "--train_file=/content/data/potter-tune/potter-micro-train.txt \\\n",
        "--validation_file=/content/data/potter-tune/potter-micro-test.txt \\\n",
        "--num_train_epochs=1 \\\n",
        "--output_dir=/content/models/gpt2-potter-micro-1ep"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-12 01:16:40.455442: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/transformers/examples/language-modeling/run_clm.py\", line 357, in <module>\n",
            "    main()\n",
            "  File \"/content/transformers/examples/language-modeling/run_clm.py\", line 153, in main\n",
            "    f\"Output directory ({training_args.output_dir}) already exists and is not empty.\"\n",
            "ValueError: Output directory (/content/models/gpt2-potter-micro-1ep) already exists and is not empty.Use --overwrite_output_dir to overcome.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZtCytiaP5rY",
        "outputId": "f1984693-5e52-4449-b843-a36439114a4a"
      },
      "source": [
        "# Generate text with a tuned model\n",
        "\n",
        "generate_text(test_prompts, ['/content/models/gpt2-potter-micro-1ep'], default_args_dict)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "PROMPT: Harry opened the door and saw\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model /content/models/gpt2-potter-micro-1ep, seq 0:\n",
            "Harry opened the door and saw none of the customers in sight. Hermione and Sirius were staying in an unusual room and not much older than the bed Draco had her share, but this\n",
            "**\n",
            "\n",
            "\n",
            "PROMPT: The New York Times reported today that\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model /content/models/gpt2-potter-micro-1ep, seq 0:\n",
            "The New York Times reported today that for the past four years, the Chief Justice had suggested that some individuals making threats are not considered \"credible threats\" because they're not being considered\n",
            "**\n",
            "\n",
            "\n",
            "PROMPT: The Daily Prophet reported today that\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model /content/models/gpt2-potter-micro-1ep, seq 0:\n",
            "The Daily Prophet reported today that none of the 21-year-olds are being accused of some wrongdoing, but she could be forced to live with it for her entire adult life.\n",
            "**\n",
            "\n",
            "\n",
            "PROMPT: Picard stepped off the shuttlecraft. He couldn't believe his eyes\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model /content/models/gpt2-potter-micro-1ep, seq 0:\n",
            "Picard stepped off the shuttlecraft. He couldn't believe his eyes, the station that had been destroyed, and had been completely destroyed, except it was not human. The space station was completely intact, and the unknown\n",
            "**\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dp6xPQq7qoT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9ff3eae-6555-4ecb-a685-9b376e55eb94"
      },
      "source": [
        "# Now let's compare the models side by side.\n",
        "\n",
        "test_prompts = [\n",
        "    'Harry opened the door and saw',\n",
        "    #'What have you done with my wand?',\n",
        "    'The New York Times reported today that',\n",
        "    'The Daily Prophet reported today that',\n",
        "    #'There was trouble afoot at Hogwarts yesterday.',\n",
        "    #'There was trouble afoot at Harvard yesterday.',\n",
        "    #'There was trouble afoot at Kennedy High School yesterday.'\n",
        "    #'Hermione was quite excited about tomorrow.',\n",
        "    #'Melissa was quite excited about tomorrow.',\n",
        "    \"Picard stepped off the shuttlecraft. He couldn't believe his eyes\",\n",
        "]\n",
        "\n",
        "models = [\n",
        "          'gpt2',\n",
        "          '/content/models/gpt2-potter-micro-1ep',\n",
        "]\n",
        "\n",
        "generate_text(test_prompts, models, default_args_dict)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "PROMPT: Harry opened the door and saw\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model gpt2, seq 0:\n",
            "Harry opened the door and saw none of the girls in the house, but had heard the voice from the edge of the window.\n",
            "\n",
            "What was all this about, Dorner\n",
            "**\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model /content/models/gpt2-potter-micro-1ep, seq 0:\n",
            "Harry opened the door and saw none of the customers in sight. Hermione and Sirius were staying in an unusual room and not much older than the bed Draco had her share, but this\n",
            "**\n",
            "\n",
            "\n",
            "PROMPT: The New York Times reported today that\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model gpt2, seq 0:\n",
            "The New York Times reported today that for the past four years, the Chief Justice had overseen the Supreme Court's quest to regulate human experimentation in the United States. A New York federal court\n",
            "**\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model /content/models/gpt2-potter-micro-1ep, seq 0:\n",
            "The New York Times reported today that for the past four years, the Chief Justice had suggested that some individuals making threats are not considered \"credible threats\" because they're not being considered\n",
            "**\n",
            "\n",
            "\n",
            "PROMPT: The Daily Prophet reported today that\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model gpt2, seq 0:\n",
            "The Daily Prophet reported today that for the past four years, the Chief Rabbi had suggested that some Jews who have not received sabbat could end up starting her wedding, but this\n",
            "**\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model /content/models/gpt2-potter-micro-1ep, seq 0:\n",
            "The Daily Prophet reported today that none of the 21-year-olds are being accused of some wrongdoing, but she could be forced to live with it for her entire adult life.\n",
            "**\n",
            "\n",
            "\n",
            "PROMPT: Picard stepped off the shuttlecraft. He couldn't believe his eyes\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model gpt2, seq 0:\n",
            "Picard stepped off the shuttlecraft. He couldn't believe his eyes, the station was in lockdown, and he had never seen anything like it before.\n",
            "\n",
            "\"Well, when you all go down, expect this\n",
            "**\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Model /content/models/gpt2-potter-micro-1ep, seq 0:\n",
            "Picard stepped off the shuttlecraft. He couldn't believe his eyes, the station that had been destroyed, and had been completely destroyed, except it was not human. The space station was completely intact, and the unknown\n",
            "**\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyDXS0FnZ_4Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b538c22d-e050-4164-df15-b145df5c8af4"
      },
      "source": [
        "# Refine the model with a little more tuning\n",
        "\n",
        "!python /content/transformers/examples/language-modeling/run_clm.py \\\n",
        "--model_type=gpt2 \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--per_device_train_batch_size=1 \\\n",
        "--per_device_eval_batch_size=1 \\\n",
        "--learning_rate 5e-5 \\\n",
        "--save_steps 5000 \\\n",
        "\\\n",
        "--model_name_or_path=/content/models/gpt2-potter-micro-1ep \\\n",
        "--train_file=/content/data/potter-tune/potter-micro-train.txt \\\n",
        "--validation_file=/content/data/potter-tune/potter-micro-test.txt \\\n",
        "--num_train_epochs=4 \\\n",
        "--output_dir=/content/models/gpt2-potter-micro-5ep"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-12 01:17:15.649353: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/transformers/examples/language-modeling/run_clm.py\", line 357, in <module>\n",
            "    main()\n",
            "  File \"/content/transformers/examples/language-modeling/run_clm.py\", line 153, in main\n",
            "    f\"Output directory ({training_args.output_dir}) already exists and is not empty.\"\n",
            "ValueError: Output directory (/content/models/gpt2-potter-micro-5ep) already exists and is not empty.Use --overwrite_output_dir to overcome.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYnRehVZuoGG",
        "outputId": "64ca24f8-5787-4c7c-b3aa-841f3dcb457c"
      },
      "source": [
        "# Try a smaller model: DistilGPT2\n",
        "\n",
        "!python /content/transformers/examples/language-modeling/run_clm.py \\\n",
        "--model_type=gpt2 \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--per_device_train_batch_size=1 \\\n",
        "--per_device_eval_batch_size=1 \\\n",
        "--learning_rate 5e-5 \\\n",
        "--save_steps 5000 \\\n",
        "\\\n",
        "--model_name_or_path=distilgpt2 \\\n",
        "--train_file=/content/data/potter-tune/potter-micro-train.txt \\\n",
        "--validation_file=/content/data/potter-tune/potter-micro-test.txt \\\n",
        "--num_train_epochs=1 \\\n",
        "--output_dir=/content/models/distilgpt2-potter-micro-1ep \\\n",
        "--overwrite_output_dir"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-12 02:05:43.438560: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "12/12/2020 02:05:45 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "12/12/2020 02:05:45 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/content/models/distilgpt2-potter-micro-1ep', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, model_parallel=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec12_02-05-45_20f9cb0d0415', logging_first_step=False, logging_steps=500, save_steps=5000, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/content/models/distilgpt2-potter-micro-1ep', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False)\n",
            "Using custom data configuration default\n",
            "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-350a34bea3087784/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
            "[INFO|configuration_utils.py:422] 2020-12-12 02:05:45,489 >> loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "[INFO|configuration_utils.py:458] 2020-12-12 02:05:45,490 >> Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:422] 2020-12-12 02:05:45,507 >> loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "[INFO|configuration_utils.py:458] 2020-12-12 02:05:45,507 >> Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1793] 2020-12-12 02:05:45,583 >> loading file https://huggingface.co/distilgpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1793] 2020-12-12 02:05:45,583 >> loading file https://huggingface.co/distilgpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1793] 2020-12-12 02:05:45,584 >> loading file https://huggingface.co/distilgpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|modeling_utils.py:1014] 2020-12-12 02:05:45,671 >> loading weights file https://huggingface.co/distilgpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba\n",
            "[INFO|modeling_utils.py:1130] 2020-12-12 02:05:49,248 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1139] 2020-12-12 02:05:49,249 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-350a34bea3087784/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-479e8bf06663729c.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-350a34bea3087784/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-f2995bb014873bcd.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-350a34bea3087784/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-8e854dd4c8390505.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-350a34bea3087784/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-1c98ab7a2c7101fc.arrow\n",
            "[INFO|trainer.py:362] 2020-12-12 02:05:55,157 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:362] 2020-12-12 02:05:55,158 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:668] 2020-12-12 02:05:55,160 >> ***** Running training *****\n",
            "[INFO|trainer.py:669] 2020-12-12 02:05:55,161 >>   Num examples = 431\n",
            "[INFO|trainer.py:670] 2020-12-12 02:05:55,161 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:671] 2020-12-12 02:05:55,161 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:672] 2020-12-12 02:05:55,161 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:673] 2020-12-12 02:05:55,161 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:674] 2020-12-12 02:05:55,161 >>   Total optimization steps = 431\n",
            "100% 431/431 [01:50<00:00,  3.89it/s][INFO|trainer.py:821] 2020-12-12 02:07:46,017 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'epoch': 1.0}\n",
            "100% 431/431 [01:50<00:00,  3.89it/s]\n",
            "[INFO|trainer.py:1183] 2020-12-12 02:07:46,035 >> Saving model checkpoint to /content/models/distilgpt2-potter-micro-1ep\n",
            "[INFO|configuration_utils.py:289] 2020-12-12 02:07:46,037 >> Configuration saved in /content/models/distilgpt2-potter-micro-1ep/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-12 02:07:47,329 >> Model weights saved in /content/models/distilgpt2-potter-micro-1ep/pytorch_model.bin\n",
            "12/12/2020 02:07:47 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:1354] 2020-12-12 02:07:47,415 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1355] 2020-12-12 02:07:47,415 >>   Num examples = 315\n",
            "[INFO|trainer.py:1356] 2020-12-12 02:07:47,415 >>   Batch size = 1\n",
            "100% 315/315 [00:25<00:00, 12.55it/s]\n",
            "12/12/2020 02:08:12 - INFO - __main__ -   ***** Eval results *****\n",
            "12/12/2020 02:08:12 - INFO - __main__ -     perplexity = 29.40990268896229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1D0S2_OBUPY5",
        "outputId": "3b6ca608-a572-44f8-f1fe-63262996d0a7"
      },
      "source": [
        "# Now gpt2 with the full data set\n",
        "\n",
        "!python /content/transformers/examples/language-modeling/run_clm.py \\\n",
        "--model_type=gpt2 \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--per_device_train_batch_size=1 \\\n",
        "--per_device_eval_batch_size=1 \\\n",
        "--learning_rate 5e-5 \\\n",
        "--save_steps 5000 \\\n",
        "\\\n",
        "--model_name_or_path=gpt2 \\\n",
        "--train_file=/content/data/potter-tune/potter-full-train.txt \\\n",
        "--validation_file=/content/data/potter-tune/potter-full-test.txt \\\n",
        "--num_train_epochs=5 \\\n",
        "--output_dir=/content/models/gpt2-potter-full-1ep \n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-12 01:17:36.444764: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "12/12/2020 01:17:38 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "12/12/2020 01:17:38 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/content/models/gpt2-potter-full-5ep', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, model_parallel=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec12_01-17-38_20f9cb0d0415', logging_first_step=False, logging_steps=2000, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=2000, dataloader_num_workers=0, past_index=-1, run_name='/content/models/gpt2-potter-full-5ep', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False)\n",
            "Using custom data configuration default\n",
            "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-d0cdf21f8981e30f/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
            "[INFO|configuration_utils.py:422] 2020-12-12 01:17:38,679 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:458] 2020-12-12 01:17:38,679 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:422] 2020-12-12 01:17:38,697 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:458] 2020-12-12 01:17:38,698 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1793] 2020-12-12 01:17:38,776 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1793] 2020-12-12 01:17:38,776 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1793] 2020-12-12 01:17:38,776 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|modeling_utils.py:1014] 2020-12-12 01:17:38,902 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1130] 2020-12-12 01:17:43,993 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1139] 2020-12-12 01:17:43,993 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "  1% 1/135 [00:00<00:18,  7.11ba/s][WARNING|tokenization_utils_base.py:3224] 2020-12-12 01:17:44,228 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1693 > 1024). Running this sequence through the model will result in indexing errors\n",
            "100% 135/135 [00:08<00:00, 16.56ba/s]\n",
            "100% 55/55 [00:03<00:00, 16.33ba/s]\n",
            "100% 135/135 [00:26<00:00,  5.18ba/s]\n",
            "100% 55/55 [00:10<00:00,  5.29ba/s]\n",
            "[INFO|trainer.py:362] 2020-12-12 01:18:37,558 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:362] 2020-12-12 01:18:37,559 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:668] 2020-12-12 01:18:37,562 >> ***** Running training *****\n",
            "[INFO|trainer.py:669] 2020-12-12 01:18:37,562 >>   Num examples = 3762\n",
            "[INFO|trainer.py:670] 2020-12-12 01:18:37,562 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:671] 2020-12-12 01:18:37,562 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:672] 2020-12-12 01:18:37,562 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:673] 2020-12-12 01:18:37,562 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:674] 2020-12-12 01:18:37,562 >>   Total optimization steps = 18810\n",
            "  3% 500/18810 [03:27<2:07:04,  2.40it/s][INFO|trainer.py:1183] 2020-12-12 01:22:05,438 >> Saving model checkpoint to /content/models/gpt2-potter-full-5ep/checkpoint-500\n",
            "[INFO|configuration_utils.py:289] 2020-12-12 01:22:05,440 >> Configuration saved in /content/models/gpt2-potter-full-5ep/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-12 01:22:07,226 >> Model weights saved in /content/models/gpt2-potter-full-5ep/checkpoint-500/pytorch_model.bin\n",
            "  5% 1000/18810 [07:06<2:03:54,  2.40it/s][INFO|trainer.py:1183] 2020-12-12 01:25:44,246 >> Saving model checkpoint to /content/models/gpt2-potter-full-5ep/checkpoint-1000\n",
            "[INFO|configuration_utils.py:289] 2020-12-12 01:25:44,248 >> Configuration saved in /content/models/gpt2-potter-full-5ep/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-12 01:25:46,155 >> Model weights saved in /content/models/gpt2-potter-full-5ep/checkpoint-1000/pytorch_model.bin\n",
            "  8% 1500/18810 [14:13<2:00:55,  2.39it/s][INFO|trainer.py:1183] 2020-12-12 01:32:50,758 >> Saving model checkpoint to /content/models/gpt2-potter-full-5ep/checkpoint-1500\n",
            "[INFO|configuration_utils.py:289] 2020-12-12 01:32:50,759 >> Configuration saved in /content/models/gpt2-potter-full-5ep/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-12 01:32:52,918 >> Model weights saved in /content/models/gpt2-potter-full-5ep/checkpoint-1500/pytorch_model.bin\n",
            "{'loss': 3.220288818359375, 'learning_rate': 4.468367889420521e-05, 'epoch': 0.531632110579479}\n",
            " 11% 2000/18810 [17:51<1:56:59,  2.39it/s][INFO|trainer.py:1183] 2020-12-12 01:36:28,610 >> Saving model checkpoint to /content/models/gpt2-potter-full-5ep/checkpoint-2000\n",
            "[INFO|configuration_utils.py:289] 2020-12-12 01:36:28,612 >> Configuration saved in /content/models/gpt2-potter-full-5ep/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-12 01:36:30,790 >> Model weights saved in /content/models/gpt2-potter-full-5ep/checkpoint-2000/pytorch_model.bin\n",
            " 13% 2500/18810 [21:29<1:53:32,  2.39it/s][INFO|trainer.py:1183] 2020-12-12 01:40:07,245 >> Saving model checkpoint to /content/models/gpt2-potter-full-5ep/checkpoint-2500\n",
            "[INFO|configuration_utils.py:289] 2020-12-12 01:40:07,246 >> Configuration saved in /content/models/gpt2-potter-full-5ep/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-12 01:40:09,524 >> Model weights saved in /content/models/gpt2-potter-full-5ep/checkpoint-2500/pytorch_model.bin\n",
            " 15% 2881/18810 [24:17<1:50:35,  2.40it/s]Traceback (most recent call last):\n",
            "  File \"/content/transformers/examples/language-modeling/run_clm.py\", line 357, in <module>\n",
            "    main()\n",
            "  File \"/content/transformers/examples/language-modeling/run_clm.py\", line 327, in main\n",
            "    trainer.train(model_path=model_path)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/trainer.py\", line 767, in train\n",
            "    tr_loss += self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/trainer.py\", line 1110, in training_step\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/tensor.py\", line 221, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\", line 132, in backward\n",
            "    allow_unreachable=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n",
            " 15% 2881/18810 [24:18<2:14:23,  1.98it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwzpottNUvpw",
        "outputId": "1d6910f6-fe6d-466f-ea83-f4b5d2fd888e"
      },
      "source": [
        "# Now gpt2-large with the full data set\n",
        "\n",
        "!python /content/transformers/examples/language-modeling/run_clm.py \\\n",
        "--model_type=gpt2 \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--per_device_train_batch_size=1 \\\n",
        "--per_device_eval_batch_size=1 \\\n",
        "--learning_rate 5e-5 \\\n",
        "--save_steps 5000 \\\n",
        "\\\n",
        "--model_name_or_path=gpt2-large \\\n",
        "--train_file=/content/data/potter-tune/potter-full-train-sents.txt \\\n",
        "--validation_file=/content/data/potter-tune/potter-full-test-sents.txt \\\n",
        "--num_train_epochs=10 \\\n",
        "--output_dir=/content/models/gpt2-large-potter-full-10ep "
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-12 02:36:29.887749: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "12/12/2020 02:36:31 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "12/12/2020 02:36:31 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/content/models/gpt2-large-potter-full-10ep', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, model_parallel=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec12_02-36-31_20f9cb0d0415', logging_first_step=False, logging_steps=500, save_steps=5000, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/content/models/gpt2-large-potter-full-10ep', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False)\n",
            "Using custom data configuration default\n",
            "Downloading and preparing dataset text/default-3f341a89aeaf9c16 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-3f341a89aeaf9c16/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-3f341a89aeaf9c16/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:422] 2020-12-12 02:36:32,318 >> loading configuration file https://huggingface.co/gpt2-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/d82fb41558a2cc40bb6e10a57bbfbd9ff2f3c6614072f05afdfa8f44d566d2ba.142693c08a15b2c586e4fcb42418d55c99b5a6a5c51228e275d9e939775865ea\n",
            "[INFO|configuration_utils.py:458] 2020-12-12 02:36:32,318 >> Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1280,\n",
            "  \"n_head\": 20,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 36,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:422] 2020-12-12 02:36:32,423 >> loading configuration file https://huggingface.co/gpt2-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/d82fb41558a2cc40bb6e10a57bbfbd9ff2f3c6614072f05afdfa8f44d566d2ba.142693c08a15b2c586e4fcb42418d55c99b5a6a5c51228e275d9e939775865ea\n",
            "[INFO|configuration_utils.py:458] 2020-12-12 02:36:32,423 >> Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1280,\n",
            "  \"n_head\": 20,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 36,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1793] 2020-12-12 02:36:32,504 >> loading file https://huggingface.co/gpt2-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/79f5e05af067df502528a0d902e82c24c3f1df9ae570c91fcc38e1f3c0af4c45.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1793] 2020-12-12 02:36:32,504 >> loading file https://huggingface.co/gpt2-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/7f7bf8a7802a708af08a812bfbdec9335f2c30f761ec14a8cd17b0d61c818876.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1793] 2020-12-12 02:36:32,504 >> loading file https://huggingface.co/gpt2-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/f1179e28982928f50ca02b0188fcd80fb4fa871ba1719df5bf81ac308d0d10af.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|modeling_utils.py:1014] 2020-12-12 02:36:32,597 >> loading weights file https://huggingface.co/gpt2-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/234578a5793e64713ba846b4c5e181e043f48b33140622e2c1dd623b665de3f9.4780ef91b17260f8dac8a3c2183aa338b27365326fb706e74db40b03749f8aba\n",
            "[INFO|modeling_utils.py:1130] 2020-12-12 02:38:27,215 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1139] 2020-12-12 02:38:27,215 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-large.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "100% 230/230 [00:09<00:00, 24.03ba/s]\n",
            "100% 93/93 [00:03<00:00, 26.05ba/s]\n",
            "100% 230/230 [00:14<00:00, 15.56ba/s]\n",
            "100% 93/93 [00:05<00:00, 15.67ba/s]\n",
            "[INFO|trainer.py:362] 2020-12-12 02:39:07,368 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:362] 2020-12-12 02:39:07,369 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:668] 2020-12-12 02:39:07,377 >> ***** Running training *****\n",
            "[INFO|trainer.py:669] 2020-12-12 02:39:07,377 >>   Num examples = 3705\n",
            "[INFO|trainer.py:670] 2020-12-12 02:39:07,377 >>   Num Epochs = 10\n",
            "[INFO|trainer.py:671] 2020-12-12 02:39:07,377 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:672] 2020-12-12 02:39:07,377 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:673] 2020-12-12 02:39:07,377 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:674] 2020-12-12 02:39:07,377 >>   Total optimization steps = 37050\n",
            "  0% 0/37050 [00:00<?, ?it/s]Traceback (most recent call last):\n",
            "  File \"/content/transformers/examples/language-modeling/run_clm.py\", line 357, in <module>\n",
            "    main()\n",
            "  File \"/content/transformers/examples/language-modeling/run_clm.py\", line 327, in main\n",
            "    trainer.train(model_path=model_path)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/trainer.py\", line 767, in train\n",
            "    tr_loss += self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/trainer.py\", line 1096, in training_step\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/trainer.py\", line 1120, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 895, in forward\n",
            "    return_dict=return_dict,\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 740, in forward\n",
            "    output_attentions=output_attentions,\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 295, in forward\n",
            "    output_attentions=output_attentions,\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 239, in forward\n",
            "    attn_outputs = self._attn(query, key, value, attention_mask, head_mask, output_attentions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 181, in _attn\n",
            "    w = self.attn_dropout(w)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/dropout.py\", line 58, in forward\n",
            "    return F.dropout(input, self.p, self.training, self.inplace)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\", line 983, in dropout\n",
            "    else _VF.dropout(input, p, training))\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.73 GiB total capacity; 12.09 GiB already allocated; 6.88 MiB free; 12.12 GiB reserved in total by PyTorch)\n",
            "  0% 0/37050 [00:00<?, ?it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8YJ8nDIsuT1"
      },
      "source": [
        "models = [\n",
        "          'gpt2',\n",
        "          '/content/models/gpt2-potter-micro-5ep',\n",
        "          '/content/models/distilgpt2-potter-micro-5ep',\n",
        "]\n",
        "\n",
        "generate_text(test_prompts, models, default_args_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHJJJwb_s6Yp",
        "outputId": "83805d09-1189-4f55-bf65-8baebeb6e98b"
      },
      "source": [
        "# This is a little gadget to break up the text files on sentences to make sure nothing goes over the GPT2 tokenizer's max length of 1024\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sents = []\n",
        "with open('/content/data/potter-tune/potter-full-test.txt') as file:\n",
        "    text = file.readlines()\n",
        "    for line in text:\n",
        "        sents += sent_tokenize(line)\n",
        "print(len(text))\n",
        "print(len(sents))\n",
        "\n",
        "with open('/content/data/potter-tune/potter-full-test-sents.txt', 'w') as outfile:\n",
        "    outfile.writelines([sent + '\\n' for sent in sents])\n",
        "\n",
        "from pprint import PrettyPrinter\n",
        "pp = PrettyPrinter(width=1024)\n",
        "pp.pprint(sents[:20])\n",
        "\n",
        "max_len = 0\n",
        "for sent in [sent + '\\n' for sent in sents]:\n",
        "    if len(sent) > max_len:\n",
        "        max_len = len(sent)\n",
        "\n",
        "print(max_len)\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "54696\n",
            "92977\n",
            "[' As per usual, Harry Potter’s day began with a visit to Diagon Alley, as it had done for the past eight years, ever since he had moved here with his godfather.',\n",
            " 'Whilst some people might have appreciated the routine, Harry found it very tiresome and longed for something interesting',\n",
            " ' to happen, like a giant beanstalk growing in the middle of the town square or maybe a dragon flying over the roof of the Leaky Cauldron.',\n",
            " 'But of course, that sort of thing only ever happened in the stories that he so eagerly devoured, and as Harry approached the town, he only saw the usual hustle and bustle of the market.',\n",
            " 'The baker with the same old bread and rolls to sell, the butcher with his alarmingly vicious-looking knife, the town physician hobbling around on her rounds…',\n",
            " '“Good morning Mr Potter!”',\n",
            " '“Morning Mr Ollivander!” Harry greeted the elderly carpenter.',\n",
            " '“Where are you off to then, hmm?”',\n",
            " '“Flourish and Blotts.',\n",
            " 'I just finished the most wonderful story!',\n",
            " 'It’s about a wizard who travels to a far-off land with a suitcase full of magical creatures -”',\n",
            " '\"Yes, yes that’s nice.',\n",
            " 'Don’t go stuffing your head with too much nonsense now.”',\n",
            " 'Harry decided that he really didn’t like Mr Ollivander all that much as he carried on walking.',\n",
            " 'Didn’t anyone here appreciate a good story?',\n",
            " 'Puzzling over this, Harry was oblivious to the strange looks and mutters that followed him around the town.',\n",
            " '\"What a strange boy.”',\n",
            " '“Head in the clouds that one.”',\n",
            " '“Shame he doesn’t fit in really.',\n",
            " 'Pretty little thing.”']\n",
            "846\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH5lTPC-ZncR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "6929c986-7d0f-4a49-b0a5-c2c38df9acb8"
      },
      "source": [
        "'''\n",
        "Notes:\n",
        "- tuning gpt2 on potter-micro (107 texts) takes about 6 minutes per epoch on a Tesla K80\n",
        "- tuning gpt2 on potter-micro (107 texts) takes about 50 minutes per epoch without a GPU\n",
        "- tuning gpt2 on potter-full (807 texts) takes about 50 minutes per epoch on a Tesla K80 - 1-epoch perplexity 18.88\n",
        "- tuning distilgpt2 on potter-full (807 texts) takes about 39 minutes per epoch on a Tesla K80 - 1-epoch perplexity 27.00\n",
        "\n",
        "\n",
        "Next steps\n",
        "- address the padding warning\n",
        "- try another model family?\n",
        "- try a template?\n",
        "'''"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nNotes:\\n- tuning gpt2 on potter-micro (107 texts) takes about 6 minutes per epoch on a Tesla K80\\n- tuning gpt2 on potter-micro (107 texts) takes about 50 minutes per epoch without a GPU\\n- tuning gpt2 on potter-full (807 texts) takes about 50 minutes per epoch on a Tesla K80 - 1-epoch perplexity 18.88\\n- tuning distilgpt2 on potter-full (807 texts) takes about 39 minutes per epoch on a Tesla K80 - 1-epoch perplexity 27.00\\n\\n\\nNext steps\\n- address the padding warning\\n- try another corpus\\n- try another model family?\\n- try a template?\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u00uj9i6t-1K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}